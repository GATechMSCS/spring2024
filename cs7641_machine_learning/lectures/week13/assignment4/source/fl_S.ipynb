{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FFHF\n",
      "FFFF\n",
      "FFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from create_env_gs import (make_env, run_ql_search, run_pi_search, run_vi_search)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Tuple, Discrete\n",
    "from bettermdptools.utils.plots import Plots\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "# make environments\n",
    "mdp_fl_S = 'FrozenLake-v1'\n",
    "size_fl_S = 4\n",
    "is_slippery_fl_S = True\n",
    "render_mode_fl_S = 'ansi'\n",
    "prob_frozen_fl_S = 0.9\n",
    "ep_steps_fl_S = 100\n",
    "frozenlakeS = make_env(mdp=mdp_fl_S, \n",
    "                      size=size_fl_S, \n",
    "                      slip=is_slippery_fl_S,\n",
    "                      render=render_mode_fl_S,\n",
    "                      prob_frozen=prob_frozen_fl_S,\n",
    "                      seed=seed,\n",
    "                      ep_steps=ep_steps_fl_S)\n",
    "\n",
    "# FL 4x4\n",
    "# QL\n",
    "iters_ql_fl_S = 100000\n",
    "gamma_ql_fl_S = [0.90, 0.99, 0.999]\n",
    "epsilon_decay_ql_fl_S = [0.80, 0.90, 0.99]\n",
    "init_alpha_ql_fl_S = [0.30, 0.50, 0.70]\n",
    "\n",
    "# PI\n",
    "iters_pi_fl_S = 100000\n",
    "gamma_pi_fl_S = [0.90, 0.99, 0.999]\n",
    "theta_pi_fl_S = [1e-5, 1e-7, 1e-9]\n",
    "\n",
    "# VI\n",
    "iters_vi_fl_S = 100000\n",
    "gamma_vi_fl_S = [0.90, 0.99, 0.999]\n",
    "theta_vi_fl_S = [1e-5, 1e-7, 1e-9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Frozen Lake 4x4}\\\\~\\\\\n",
    "\\textbf{Q Learning}\\\\\n",
    "\\textbf{Gamma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_learning: gamma=0.9; edr=0.9; ialpha=0.5; episodes=100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo_leads/miniconda3/envs/mlmdp/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/home/leonardo_leads/miniconda3/envs/mlmdp/lib/python3.12/site-packages/bettermdptools/algorithms/rl.py:178: UserWarning: Episode was truncated.  Bootstrapping 0 reward.\n",
      "  warnings.warn(\"Episode was truncated.  Bootstrapping 0 reward.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime = 39.61 seconds\n",
      "Avg. episode reward:  0.9989654534999999\n",
      "###################\n",
      "\n",
      "q_learning: gamma=0.99; edr=0.9; ialpha=0.5; episodes=100000\n",
      "runtime = 38.16 seconds\n",
      "Avg. episode reward:  0.9988951035\n",
      "###################\n",
      "\n",
      "q_learning: gamma=0.999; edr=0.9; ialpha=0.5; episodes=100000\n",
      "runtime = 45.38 seconds\n",
      "Avg. episode reward:  0.9986743170000002\n",
      "###################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0.9, 0.99, 0.999])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ql_fl_S_gamma = run_ql_search(process=frozenlakeS,\n",
    "                        gamma=gamma_ql_fl_S,\n",
    "                        n_episodes=iters_ql_fl_S)\n",
    "ql_fl_S_gamma.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Q Learning}\\\\\n",
    "\\textbf{Epsilon Decay Ratio}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_learning: epsilon_decay_ratio=0.8; gamma=0.99; ialpha=0.5; episodes=100000\n",
      "runtime = 37.82 seconds\n",
      "Avg. episode reward:  0.9989654570000001\n",
      "###################\n",
      "\n",
      "q_learning: epsilon_decay_ratio=0.9; gamma=0.99; ialpha=0.5; episodes=100000\n",
      "runtime = 37.83 seconds\n",
      "Avg. episode reward:  0.9988951035\n",
      "###################\n",
      "\n",
      "q_learning: epsilon_decay_ratio=0.99; gamma=0.99; ialpha=0.5; episodes=100000\n",
      "runtime = 38.76 seconds\n",
      "Avg. episode reward:  0.9989652690000002\n",
      "###################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0.8, 0.9, 0.99])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ql_fl_S_edr = run_ql_search(process=frozenlakeS,\n",
    "                        epsilon_decay_ratio=epsilon_decay_ql_fl_S,\n",
    "                        n_episodes=iters_ql_fl_S)\n",
    "ql_fl_S_edr.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Q Learning}\\\\\n",
    "\\textbf{Init Alpha}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_learning: init_alpha=0.3; gamma=0.99; edr=0.9; episodes=100000\n",
      "runtime = 37.57 seconds\n",
      "Avg. episode reward:  0.9988950645\n",
      "###################\n",
      "\n",
      "q_learning: init_alpha=0.5; gamma=0.99; edr=0.9; episodes=100000\n",
      "runtime = 33.88 seconds\n",
      "Avg. episode reward:  0.9988951035\n",
      "###################\n",
      "\n",
      "q_learning: init_alpha=0.7; gamma=0.99; edr=0.9; episodes=100000\n",
      "runtime = 37.02 seconds\n",
      "Avg. episode reward:  0.9989651335\n",
      "###################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0.3, 0.5, 0.7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ql_fl_S_alpha = run_ql_search(process=frozenlakeS,\n",
    "                        init_alpha=init_alpha_ql_fl_S,\n",
    "                        n_episodes=iters_ql_fl_S)\n",
    "ql_fl_S_alpha.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Policy Iteration}\\\\\n",
    "\\textbf{Gamma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI: gamma=0.9; theta=1e-10; iters=100000\n",
      "runtime = 0.01 seconds\n",
      "Avg. episode reward:  0.99894572\n",
      "###################\n",
      "\n",
      "PI: gamma=0.99; theta=1e-10; iters=100000\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  0.99894572\n",
      "###################\n",
      "\n",
      "PI: gamma=0.999; theta=1e-10; iters=100000\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  0.99894572\n",
      "###################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0.9, 0.99, 0.999])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_S_pi_gam = run_pi_search(process=frozenlakeS,\n",
    "                  gamma=gamma_pi_fl_S,\n",
    "                  n_iters=iters_pi_fl_S)\n",
    "fl_S_pi_gam.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Policy Iteration}\\\\\n",
    "\\textbf{Theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI: theta=1e-05; gamma=1.0; iters=100000\n",
      "runtime = 0.01 seconds\n",
      "Avg. episode reward:  0.9990088735\n",
      "###################\n",
      "\n",
      "PI: theta=1e-07; gamma=1.0; iters=100000\n",
      "runtime = 0.01 seconds\n",
      "Avg. episode reward:  0.9990088735\n",
      "###################\n",
      "\n",
      "PI: theta=1e-09; gamma=1.0; iters=100000\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  0.9990088735\n",
      "###################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([1e-05, 1e-07, 1e-09])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_S_pi_theta = run_pi_search(process=frozenlakeS,\n",
    "                  theta=theta_pi_fl_S,\n",
    "                  n_iters=iters_pi_fl_S)\n",
    "fl_S_pi_theta.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Value Iteration}\\\\\n",
    "\\textbf{Gamma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VI: gamma=0.9; theta=1e-10; iters=100000\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  0.99894572\n",
      "###################\n",
      "\n",
      "VI: gamma=0.99; theta=1e-10; iters=100000\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.99894572\n",
      "###################\n",
      "\n",
      "VI: gamma=0.999; theta=1e-10; iters=100000\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.99894572\n",
      "###################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([0.9, 0.99, 0.999])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_S_vi_gam = run_vi_search(process=frozenlakeS,\n",
    "                  gamma=gamma_vi_fl_S,\n",
    "                  n_iters=iters_vi_fl_S)\n",
    "fl_S_vi_gam.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,) {0: 1, 1: 2, 2: 3, 3: 2, 4: 1, 5: 0, 6: 0, 7: 2, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 2, 14: 2, 15: 0} (100000, 16)\n"
     ]
    }
   ],
   "source": [
    "V_vi_gamma = fl_S_vi_gam[0.9]['V']\n",
    "pi_vi_gamma = fl_S_vi_gam[0.9]['pi']\n",
    "v_track_vi_gamma = fl_S_vi_gam[0.9]['vi_track']\n",
    "print(V_vi_gamma.shape, pi_vi_gamma, v_track_vi_gamma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size=(4,4)\n",
    "# Plots.values_heat_map(V_vi_gamma, \"Frozen Lake\\nValue Iteration State Values\", size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_value_per_iter = np.trim_zeros(np.mean(v_track_vi_gamma, axis=1), 'b')\n",
    "# Plots.v_iters_plot(max_value_per_iter, \"Frozen Lake\\nMean Value v Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fl_actions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "# fl_map_size=(4, 4)\n",
    "# title=\"FL Mapped Policy\\nArrows represent best action\"\n",
    "# val_max, policy_map = Plots.get_policy_map(pi_vi_gamma, V_vi_gamma, fl_actions, fl_map_size)\n",
    "# Plots.plot_policy(val_max, policy_map, fl_map_size, title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Value Iteration}\\\\\n",
    "\\textbf{Theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VI: theta=1e-05; gamma=1.0; iters=100000\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.9990088735\n",
      "###################\n",
      "\n",
      "VI: theta=1e-07; gamma=1.0; iters=100000\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  0.9990088735\n",
      "###################\n",
      "\n",
      "VI: theta=1e-09; gamma=1.0; iters=100000\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.9990088735\n",
      "###################\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys([1e-05, 1e-07, 1e-09])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_S_vi_theta = run_vi_search(process=frozenlakeS,\n",
    "                  theta=theta_vi_fl_S,\n",
    "                  n_iters=iters_vi_fl_S)\n",
    "fl_S_vi_theta.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
