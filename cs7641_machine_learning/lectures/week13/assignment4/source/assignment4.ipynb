{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_env_gs import (make_env, run_q_gridsearch, run_pi_gs, run_vi_gs)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Tuple, Discrete\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Frozen Lake}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_fl = 'FrozenLake8x8-v1'\n",
    "size_fl = 16\n",
    "is_slippery_fl = True\n",
    "render_mode_fl = 'ansi'\n",
    "prob_frozen_fl = 0.9\n",
    "ep_steps_fl = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFHFFFFFFFFF\n",
      "FFFFFFFFFFFFFFFF\n",
      "FFFFFFHFFFFFFFFH\n",
      "FFFFFFFFFFFFFFFF\n",
      "FFFFFFFFFFFFFFFF\n",
      "FFFFHFFFFFFHFFFF\n",
      "FFFFFFFFFFFFFFFF\n",
      "FFFFFFFHHFFFHHFF\n",
      "FFFFFFHFFFFFFFHF\n",
      "FFFFFFFFFFFFFFFF\n",
      "FFFFFFFHFFFFFFHF\n",
      "FFFFFFFFFFFFHFFF\n",
      "FFFFFFFFFFFHFFFF\n",
      "FFFFFFFFFFFFFHFH\n",
      "HFFFFFFFFFFFFHFF\n",
      "FFHFFFHFFFFFFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frozenlake = make_env(mdp=mdp_fl, \n",
    "                      size=size_fl, \n",
    "                      slip=is_slippery_fl,\n",
    "                      render=render_mode_fl,\n",
    "                      prob_frozen=prob_frozen_fl,\n",
    "                      seed=seed,\n",
    "                      ep_steps=ep_steps_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_q_fl = [0.90, 0.99, 0.999]\n",
    "epsilon_decay_q_fl = [0.90, 0.99, 0.999]\n",
    "iters_q_fl = [100000, 300000, 500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running q_learning with gamma: 0.9 epsilon decay: 0.9  iterations: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo_leads/miniconda3/envs/mlmdp/lib/python3.12/site-packages/bettermdptools/algorithms/rl.py:178: UserWarning: Episode was truncated.  Bootstrapping 0 reward.\n",
      "  warnings.warn(\"Episode was truncated.  Bootstrapping 0 reward.\")\n"
     ]
    }
   ],
   "source": [
    "q_fl = run_q_gridsearch(process=frozenlake,\n",
    "                        gamma=gamma_q_fl,\n",
    "                        epsilon_decay=epsilon_decay_q_fl,\n",
    "                        iters=iters_q_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_fl.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_reward_one = {key: value for key, value in q_fl[(1.05, 1.0, 50000)].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in q_fl[(1.05, 1.0, 50000)].items():\n",
    "    # if key == 'average episode rewards':\n",
    "    #     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_fl[(1.05, 1.0, 50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_p_fl = [0.90, 0.95, 0.99, 0.999]\n",
    "theta_p_fl = [0.01, 0.001, .0001, 0.00001]\n",
    "iters_p_fl = [100000, 400000, 700000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.15 seconds\n",
      "Avg. episode reward:  0.8014665\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo_leads/miniconda3/envs/mlmdp/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime = 0.16 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.17 seconds\n",
      "Avg. episode reward:  0.8625149999999999\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.8014665\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.16 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.18 seconds\n",
      "Avg. episode reward:  0.8625149999999999\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.8014665\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.16 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.18 seconds\n",
      "Avg. episode reward:  0.8625149999999999\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.8014665\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.16 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.18 seconds\n",
      "Avg. episode reward:  0.8625149999999999\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.17 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.18 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.24 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.29 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.15 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.19 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.26 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.20 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.19 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.26 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.19 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.19 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.25 seconds\n",
      "Avg. episode reward:  0.8026775000000002\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.12 seconds\n",
      "Avg. episode reward:  0.7927815000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.44 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.46 seconds\n",
      "Avg. episode reward:  0.9620990000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.80 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.15 seconds\n",
      "Avg. episode reward:  0.7927815000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.49 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.44 seconds\n",
      "Avg. episode reward:  0.9620990000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.80 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.7927815000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.45 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.42 seconds\n",
      "Avg. episode reward:  0.9620990000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.71 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.12 seconds\n",
      "Avg. episode reward:  0.7927815000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.42 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.43 seconds\n",
      "Avg. episode reward:  0.9620990000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.75 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.9610889999999999\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.38 seconds\n",
      "Avg. episode reward:  0.9712519999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.0001\n",
      "runtime = 1.16 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 1e-05\n",
      "runtime = 2.49 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.16 seconds\n",
      "Avg. episode reward:  0.9610889999999999\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.53 seconds\n",
      "Avg. episode reward:  0.9712519999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.0001\n",
      "runtime = 1.93 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 1e-05\n",
      "runtime = 2.28 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.15 seconds\n",
      "Avg. episode reward:  0.9610889999999999\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.34 seconds\n",
      "Avg. episode reward:  0.9712519999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.0001\n",
      "runtime = 1.24 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 1e-05\n",
      "runtime = 2.33 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.9610889999999999\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.33 seconds\n",
      "Avg. episode reward:  0.9712519999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 1.24 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 2.27 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fl_pi = run_pi_gs(process=frozenlake,\n",
    "                  gamma=gamma_p_fl,\n",
    "                  theta=theta_p_fl,\n",
    "                  iters=iters_p_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0.9, 100000, 0.01), (0.9, 100000, 0.001), (0.9, 100000, 0.0001), (0.9, 100000, 1e-05), (0.9, 400000, 0.01), (0.9, 400000, 0.001), (0.9, 400000, 0.0001), (0.9, 400000, 1e-05), (0.9, 700000, 0.01), (0.9, 700000, 0.001), (0.9, 700000, 0.0001), (0.9, 700000, 1e-05), (0.9, 1000000, 0.01), (0.9, 1000000, 0.001), (0.9, 1000000, 0.0001), (0.9, 1000000, 1e-05), (0.95, 100000, 0.01), (0.95, 100000, 0.001), (0.95, 100000, 0.0001), (0.95, 100000, 1e-05), (0.95, 400000, 0.01), (0.95, 400000, 0.001), (0.95, 400000, 0.0001), (0.95, 400000, 1e-05), (0.95, 700000, 0.01), (0.95, 700000, 0.001), (0.95, 700000, 0.0001), (0.95, 700000, 1e-05), (0.95, 1000000, 0.01), (0.95, 1000000, 0.001), (0.95, 1000000, 0.0001), (0.95, 1000000, 1e-05), (0.99, 100000, 0.01), (0.99, 100000, 0.001), (0.99, 100000, 0.0001), (0.99, 100000, 1e-05), (0.99, 400000, 0.01), (0.99, 400000, 0.001), (0.99, 400000, 0.0001), (0.99, 400000, 1e-05), (0.99, 700000, 0.01), (0.99, 700000, 0.001), (0.99, 700000, 0.0001), (0.99, 700000, 1e-05), (0.99, 1000000, 0.01), (0.99, 1000000, 0.001), (0.99, 1000000, 0.0001), (0.99, 1000000, 1e-05), (0.999, 100000, 0.01), (0.999, 100000, 0.001), (0.999, 100000, 0.0001), (0.999, 100000, 1e-05), (0.999, 400000, 0.01), (0.999, 400000, 0.001), (0.999, 400000, 0.0001), (0.999, 400000, 1e-05), (0.999, 700000, 0.01), (0.999, 700000, 0.001), (0.999, 700000, 0.0001), (0.999, 700000, 1e-05), (0.999, 1000000, 0.01), (0.999, 1000000, 0.001), (0.999, 1000000, 0.0001), (0.999, 1000000, 1e-05)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_pi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_v_fl = [0.90, 0.95, 0.99, 0.999]\n",
    "theta_v_fl = [0.01, 0.001, .0001, 0.00001]\n",
    "iters_v_fl = [100000, 400000, 700000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.013396499999999947\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  -0.003358999999999946\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.09 seconds\n",
      "Avg. episode reward:  0.398001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.013396499999999947\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  -0.003358999999999946\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.09 seconds\n",
      "Avg. episode reward:  0.398001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.013396499999999947\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  -0.003358999999999946\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.398001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.15 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.013396499999999947\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  -0.003358999999999946\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.398001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.14 seconds\n",
      "Avg. episode reward:  0.811679\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.013410499999999947\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.09 seconds\n",
      "Avg. episode reward:  0.8915215\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.17 seconds\n",
      "Avg. episode reward:  0.9622090000000003\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.25 seconds\n",
      "Avg. episode reward:  0.9622090000000003\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.013410499999999947\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.11 seconds\n",
      "Avg. episode reward:  0.8915215\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.20 seconds\n",
      "Avg. episode reward:  0.9622090000000003\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.26 seconds\n",
      "Avg. episode reward:  0.9622090000000003\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.013410499999999947\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.8915215\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.19 seconds\n",
      "Avg. episode reward:  0.9622090000000003\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.27 seconds\n",
      "Avg. episode reward:  0.9622090000000003\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.013410499999999947\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.09 seconds\n",
      "Avg. episode reward:  0.8915215\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.18 seconds\n",
      "Avg. episode reward:  0.9622090000000003\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.26 seconds\n",
      "Avg. episode reward:  0.9622090000000003\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  -0.013444499999999948\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.33 seconds\n",
      "Avg. episode reward:  0.9422734999999999\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.54 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.72 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  -0.013444499999999948\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.33 seconds\n",
      "Avg. episode reward:  0.9422734999999999\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.56 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.75 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  -0.013444499999999948\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.31 seconds\n",
      "Avg. episode reward:  0.9422734999999999\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.53 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.74 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  -0.013444499999999948\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.32 seconds\n",
      "Avg. episode reward:  0.9422734999999999\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.55 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.71 seconds\n",
      "Avg. episode reward:  0.9321605000000001\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.8331234999999999\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.61 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.99 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 1e-05\n",
      "runtime = 1.37 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.8331234999999999\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.59 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.98 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 1e-05\n",
      "runtime = 1.37 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.8331234999999999\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.60 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.96 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 1e-05\n",
      "runtime = 1.36 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.8331234999999999\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.59 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 1.00 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 1.39 seconds\n",
      "Avg. episode reward:  0.9713074999999998\n",
      "###################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fl_vi = run_vi_gs(process=frozenlake,\n",
    "                  gamma=gamma_v_fl,\n",
    "                  theta=theta_v_fl,\n",
    "                  iters=iters_v_fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0.9, 100000, 0.0001), (0.9, 100000, 1e-05), (0.9, 400000, 0.0001), (0.9, 400000, 1e-05), (0.9, 700000, 0.0001), (0.9, 700000, 1e-05), (0.9, 1000000, 0.0001), (0.9, 1000000, 1e-05), (0.95, 100000, 0.001), (0.95, 100000, 0.0001), (0.95, 100000, 1e-05), (0.95, 400000, 0.001), (0.95, 400000, 0.0001), (0.95, 400000, 1e-05), (0.95, 700000, 0.001), (0.95, 700000, 0.0001), (0.95, 700000, 1e-05), (0.95, 1000000, 0.001), (0.95, 1000000, 0.0001), (0.95, 1000000, 1e-05), (0.99, 100000, 0.001), (0.99, 100000, 0.0001), (0.99, 100000, 1e-05), (0.99, 400000, 0.001), (0.99, 400000, 0.0001), (0.99, 400000, 1e-05), (0.99, 700000, 0.001), (0.99, 700000, 0.0001), (0.99, 700000, 1e-05), (0.99, 1000000, 0.001), (0.99, 1000000, 0.0001), (0.99, 1000000, 1e-05), (0.999, 100000, 0.01), (0.999, 100000, 0.001), (0.999, 100000, 0.0001), (0.999, 100000, 1e-05), (0.999, 400000, 0.01), (0.999, 400000, 0.001), (0.999, 400000, 0.0001), (0.999, 400000, 1e-05), (0.999, 700000, 0.01), (0.999, 700000, 0.001), (0.999, 700000, 0.0001), (0.999, 700000, 1e-05), (0.999, 1000000, 0.01), (0.999, 1000000, 0.001), (0.999, 1000000, 0.0001), (0.999, 1000000, 1e-05)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fl_vi.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Blackjack}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_bj = 'Blackjack-v1'\n",
    "render_mode_bj = 'rgb_array'\n",
    "size_bj = Tuple([Discrete(32), Discrete(11), Discrete(2)])\n",
    "ep_steps_bj = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  ...\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]]\n",
      "\n",
      " [[ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  ...\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]]\n",
      "\n",
      " [[ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  ...\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  ...\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]]\n",
      "\n",
      " [[ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  ...\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]]\n",
      "\n",
      " [[ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  ...\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]\n",
      "  [ 7 99 36]]]\n"
     ]
    }
   ],
   "source": [
    "blackjack = make_env(mdp=mdp_bj,\n",
    "                     size=size_bj,\n",
    "                     slip=None,\n",
    "                     render=render_mode_bj,\n",
    "                     seed=seed,\n",
    "                     prob_frozen=None,\n",
    "                     ep_steps=ep_steps_bj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_q_bj = [0.90, 0.99, 0.999]\n",
    "epsilon_decay_q_bj = [0.90, 0.99, 0.999]\n",
    "iters_q_bj = [100000, 300000, 500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_bj = run_q_gridsearch(process=blackjack,\n",
    "                        gamma=gamma_q_bj,\n",
    "                        epsilon_decay=epsilon_decay_q_bj,\n",
    "                        iters=iters_q_bj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_bj.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_reward_one = {key: value for key, value in q_bj[(1.05, 1.0, 50000)].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in q_bj[(1.05, 1.0, 50000)].items():\n",
    "    # if key == 'average episode rewards':\n",
    "    #     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_bj[(1.05, 1.0, 50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_p_bj = [0.90, 0.95, 0.99, 0.999]\n",
    "theta_p_bj = [0.01, 0.001, .0001, 0.00001]\n",
    "iters_p_bj = [100000, 400000, 700000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bj_pi = run_pi_gs(process=blackjack,\n",
    "                  gamma=gamma_p_bj,\n",
    "                  theta=theta_p_bj,\n",
    "                  iters=iters_p_bj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0.9, 100000, 0.01), (0.9, 100000, 0.001), (0.9, 100000, 0.0001), (0.9, 100000, 1e-05), (0.9, 400000, 0.01), (0.9, 400000, 0.001), (0.9, 400000, 0.0001), (0.9, 400000, 1e-05), (0.9, 700000, 0.01), (0.9, 700000, 0.001), (0.9, 700000, 0.0001), (0.9, 700000, 1e-05), (0.9, 1000000, 0.01), (0.9, 1000000, 0.001), (0.9, 1000000, 0.0001), (0.9, 1000000, 1e-05), (0.95, 100000, 0.01), (0.95, 100000, 0.001), (0.95, 100000, 0.0001), (0.95, 100000, 1e-05), (0.95, 400000, 0.01), (0.95, 400000, 0.001), (0.95, 400000, 0.0001), (0.95, 400000, 1e-05), (0.95, 700000, 0.01), (0.95, 700000, 0.001), (0.95, 700000, 0.0001), (0.95, 700000, 1e-05), (0.95, 1000000, 0.01), (0.95, 1000000, 0.001), (0.95, 1000000, 0.0001), (0.95, 1000000, 1e-05), (0.99, 100000, 0.01), (0.99, 100000, 0.001), (0.99, 100000, 0.0001), (0.99, 100000, 1e-05), (0.99, 400000, 0.01), (0.99, 400000, 0.001), (0.99, 400000, 0.0001), (0.99, 400000, 1e-05), (0.99, 700000, 0.01), (0.99, 700000, 0.001), (0.99, 700000, 0.0001), (0.99, 700000, 1e-05), (0.99, 1000000, 0.01), (0.99, 1000000, 0.001), (0.99, 1000000, 0.0001), (0.99, 1000000, 1e-05), (0.999, 100000, 0.01), (0.999, 100000, 0.001), (0.999, 100000, 0.0001), (0.999, 100000, 1e-05), (0.999, 400000, 0.01), (0.999, 400000, 0.001), (0.999, 400000, 0.0001), (0.999, 400000, 1e-05), (0.999, 700000, 0.01), (0.999, 700000, 0.001), (0.999, 700000, 0.0001), (0.999, 700000, 1e-05), (0.999, 1000000, 0.01), (0.999, 1000000, 0.001), (0.999, 1000000, 0.0001), (0.999, 1000000, 1e-05)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bj_pi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_v_bj = [0.90, 0.95, 0.99, 0.999]\n",
    "theta_v_bj = [0.01, 0.001, .0001, 0.00001]\n",
    "iters_v_bj = [100000, 400000, 700000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.03 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.03 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.02 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.09\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.02 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.02 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.02 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.02 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.02 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.02 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.02 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.01 seconds\n",
      "in bj****************************\n",
      "Avg. episode reward:  -0.07\n",
      "###################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bj_vi = run_vi_gs(process=blackjack,\n",
    "                  gamma=gamma_v_bj,\n",
    "                  theta=theta_v_bj,\n",
    "                  iters=iters_v_bj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0.9, 100000, 0.01), (0.9, 100000, 0.001), (0.9, 100000, 0.0001), (0.9, 100000, 1e-05), (0.9, 400000, 0.01), (0.9, 400000, 0.001), (0.9, 400000, 0.0001), (0.9, 400000, 1e-05), (0.9, 700000, 0.01), (0.9, 700000, 0.001), (0.9, 700000, 0.0001), (0.9, 700000, 1e-05), (0.9, 1000000, 0.01), (0.9, 1000000, 0.001), (0.9, 1000000, 0.0001), (0.9, 1000000, 1e-05), (0.95, 100000, 0.01), (0.95, 100000, 0.001), (0.95, 100000, 0.0001), (0.95, 100000, 1e-05), (0.95, 400000, 0.01), (0.95, 400000, 0.001), (0.95, 400000, 0.0001), (0.95, 400000, 1e-05), (0.95, 700000, 0.01), (0.95, 700000, 0.001), (0.95, 700000, 0.0001), (0.95, 700000, 1e-05), (0.95, 1000000, 0.01), (0.95, 1000000, 0.001), (0.95, 1000000, 0.0001), (0.95, 1000000, 1e-05), (0.99, 100000, 0.01), (0.99, 100000, 0.001), (0.99, 100000, 0.0001), (0.99, 100000, 1e-05), (0.99, 400000, 0.01), (0.99, 400000, 0.001), (0.99, 400000, 0.0001), (0.99, 400000, 1e-05), (0.99, 700000, 0.01), (0.99, 700000, 0.001), (0.99, 700000, 0.0001), (0.99, 700000, 1e-05), (0.99, 1000000, 0.01), (0.99, 1000000, 0.001), (0.99, 1000000, 0.0001), (0.99, 1000000, 1e-05), (0.999, 100000, 0.01), (0.999, 100000, 0.001), (0.999, 100000, 0.0001), (0.999, 100000, 1e-05), (0.999, 400000, 0.01), (0.999, 400000, 0.001), (0.999, 400000, 0.0001), (0.999, 400000, 1e-05), (0.999, 700000, 0.01), (0.999, 700000, 0.001), (0.999, 700000, 0.0001), (0.999, 700000, 1e-05), (0.999, 1000000, 0.01), (0.999, 1000000, 0.001), (0.999, 1000000, 0.0001), (0.999, 1000000, 1e-05)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bj_vi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
