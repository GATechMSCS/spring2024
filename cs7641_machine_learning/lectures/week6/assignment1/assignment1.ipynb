{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# statistical tests\n",
    "import scipy as sp\n",
    "\n",
    "# visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# model preparation\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
    "                                     GridSearchCV, RandomizedSearchCV)\n",
    "\n",
    "# pytorch\n",
    "import torch as pt\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import (classification_report, ConfusionMatrixDisplay,\n",
    "                            RocCurveDisplay, accuracy_score, f1_score,\n",
    "                            precision_score, recall_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fil1 = \"\"\n",
    "df1 = pd.read_csv()\n",
    "\n",
    "fil2 = \"\"\n",
    "df2 = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.head(3))\n",
    "print(df2.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.info())\n",
    "print(df2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import tqdm\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Read data\n",
    "# data = pd.read_csv(\"sonar.csv\", header=None)\n",
    "# X = data.iloc[:, 0:60]\n",
    "# y = data.iloc[:, 60]\n",
    "\n",
    "# # Binary encoding of labels\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(y)\n",
    "# y = encoder.transform(y)\n",
    "\n",
    "# # Convert to 2D PyTorch tensors\n",
    "# X = torch.tensor(X.values, dtype=torch.float32)\n",
    "# y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# # Define two models\n",
    "# class Wide(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.hidden = nn.Linear(60, 180)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.output = nn.Linear(180, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.hidden(x))\n",
    "#         x = self.sigmoid(self.output(x))\n",
    "#         return x\n",
    "\n",
    "# class Deep(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = nn.Linear(60, 60)\n",
    "#         self.act1 = nn.ReLU()\n",
    "#         self.layer2 = nn.Linear(60, 60)\n",
    "#         self.act2 = nn.ReLU()\n",
    "#         self.layer3 = nn.Linear(60, 60)\n",
    "#         self.act3 = nn.ReLU()\n",
    "#         self.output = nn.Linear(60, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.act1(self.layer1(x))\n",
    "#         x = self.act2(self.layer2(x))\n",
    "#         x = self.act3(self.layer3(x))\n",
    "#         x = self.sigmoid(self.output(x))\n",
    "#         return x\n",
    "\n",
    "# # Compare model sizes\n",
    "# model1 = Wide()\n",
    "# model2 = Deep()\n",
    "# print(sum([x.reshape(-1).shape[0] for x in model1.parameters()]))  # 11161\n",
    "# print(sum([x.reshape(-1).shape[0] for x in model2.parameters()]))  # 11041\n",
    "\n",
    "# # Helper function to train one model\n",
    "# def model_train(model, X_train, y_train, X_val, y_val):\n",
    "#     # loss function and optimizer\n",
    "#     loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#     n_epochs = 300   # number of epochs to run\n",
    "#     batch_size = 10  # size of each batch\n",
    "#     batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "#     # Hold the best model\n",
    "#     best_acc = - np.inf   # init to negative infinity\n",
    "#     best_weights = None\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         model.train()\n",
    "#         with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "#             bar.set_description(f\"Epoch {epoch}\")\n",
    "#             for start in bar:\n",
    "#                 # take a batch\n",
    "#                 X_batch = X_train[start:start+batch_size]\n",
    "#                 y_batch = y_train[start:start+batch_size]\n",
    "#                 # forward pass\n",
    "#                 y_pred = model(X_batch)\n",
    "#                 loss = loss_fn(y_pred, y_batch)\n",
    "#                 # backward pass\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 # update weights\n",
    "#                 optimizer.step()\n",
    "#                 # print progress\n",
    "#                 acc = (y_pred.round() == y_batch).float().mean()\n",
    "#                 bar.set_postfix(\n",
    "#                     loss=float(loss),\n",
    "#                     acc=float(acc)\n",
    "#                 )\n",
    "#         # evaluate accuracy at end of each epoch\n",
    "#         model.eval()\n",
    "#         y_pred = model(X_val)\n",
    "#         acc = (y_pred.round() == y_val).float().mean()\n",
    "#         acc = float(acc)\n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_weights = copy.deepcopy(model.state_dict())\n",
    "#     # restore model and return best accuracy\n",
    "#     model.load_state_dict(best_weights)\n",
    "#     return best_acc\n",
    "\n",
    "# # train-test split: Hold out the test set for final model evaluation\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "# # define 5-fold cross validation test harness\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# cv_scores_wide = []\n",
    "# for train, test in kfold.split(X_train, y_train):\n",
    "#     # create model, train, and get accuracy\n",
    "#     model = Wide()\n",
    "#     acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "#     print(\"Accuracy (wide): %.2f\" % acc)\n",
    "#     cv_scores_wide.append(acc)\n",
    "# cv_scores_deep = []\n",
    "# for train, test in kfold.split(X_train, y_train):\n",
    "#     # create model, train, and get accuracy\n",
    "#     model = Deep()\n",
    "#     acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "#     print(\"Accuracy (deep): %.2f\" % acc)\n",
    "#     cv_scores_deep.append(acc)\n",
    "\n",
    "# # evaluate the model\n",
    "# wide_acc = np.mean(cv_scores_wide)\n",
    "# wide_std = np.std(cv_scores_wide)\n",
    "# deep_acc = np.mean(cv_scores_deep)\n",
    "# deep_std = np.std(cv_scores_deep)\n",
    "# print(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\n",
    "# print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))\n",
    "\n",
    "# # rebuild model with full set of training data\n",
    "# if wide_acc > deep_acc:\n",
    "#     print(\"Retrain a wide model\")\n",
    "#     model = Wide()\n",
    "# else:\n",
    "#     print(\"Retrain a deep model\")\n",
    "#     model = Deep()\n",
    "# acc = model_train(model, X_train, y_train, X_test, y_test)\n",
    "# print(f\"Final model accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Test out inference with 5 samples\n",
    "#     for i in range(5):\n",
    "#         y_pred = model(X_test[i:i+1])\n",
    "#         print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n",
    "\n",
    "#     # Plot the ROC curve\n",
    "#     y_pred = model(X_test)\n",
    "#     fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "#     plt.plot(fpr, tpr) # ROC curve = TPR vs FPR\n",
    "#     plt.title(\"Receiver Operating Characteristics\")\n",
    "#     plt.xlabel(\"False Positive Rate\")\n",
    "#     plt.ylabel(\"True Positive Rate\")\n",
    "#     plt.show()\n",
    "\n",
    "### LOGISTIC REGRESSION\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from sklearn import datasets\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# nn = torch.nn\n",
    "\n",
    "# # 0) prepare data\n",
    "# bc = datasets.load_breast_cancer()\n",
    "# x, y = bc.data, bc.target\n",
    "\n",
    "# n_samples, n_features = x.shape\n",
    "# print(n_samples, n_features)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=1234)\n",
    "\n",
    "# # scale\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "# X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "# y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "# y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "# y_train = y_train.view(y_train.shape[0], 1)\n",
    "# y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# # 1) model\n",
    "# # f = wx + b, sigmoid at the end\n",
    "# class LogisticRegression(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_input_features):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#         self.linear = nn.Linear(n_input_features, out_features=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y_pred = torch.sigmoid(self.linear(x))\n",
    "#         return y_pred\n",
    "\n",
    "# model = LogisticRegression(n_features)\n",
    "\n",
    "# # # 2) loss and optimizer\n",
    "# lr = .01\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# # # 3) training loop\n",
    "# n_epochs = 100\n",
    "# for epoch in range(n_epochs):\n",
    "#     # forward pass and loss\n",
    "#     y_pred = model(X_train)\n",
    "#     loss = criterion(y_pred, y_train)\n",
    "\n",
    "#     # backward pass\n",
    "#     loss.backward()\n",
    "\n",
    "#     # update\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # zero gradients\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f\"epoch: {epoch+1}, loss = {loss.item()}\")\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     y_predicted = model(X_test)\n",
    "#     y_predicted_cls = y_predicted.round()\n",
    "#     acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "#     print(f\"accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_models = {'decision tree':{'model': DecisionTreeClassifier(max_depth=7,\n",
    "                                                                 random_state=rand)},\n",
    "                'knn':{'model': KNeighborsClassifier(n_neighbors=7)},\n",
    "                'gradient boosting':{'model': GradientBoostingClassifier(n_estimators=210)},\n",
    "                'support vector machine': {'model': SVC()},\n",
    "                'mlp':{'model':make_pipeline(StandardScaler(),\n",
    "                                             MLPClassifier(hidden_layer_sizes=(7,),\n",
    "                                                           max_iter=500,\n",
    "                                                           early_stopping=True,\n",
    "                                                           random_state=rand))}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in class_models.keys():\n",
    "    fitted_model = class_models[model_name]['model'].fit(X_train, y_train_class)\n",
    "    y_train_pred = fitted_model.predict(X_train.values)\n",
    "    y_test_prob = fitted_model.predict_proba(X_test.values)[:,1]\n",
    "    y_test_pred = np.where(y_test_prob > 0.5, 1, 0)\n",
    "    class_models[model_name]['fitted'] = fitted_model\n",
    "    class_models[model_name]['probs'] = y_test_prob\n",
    "    class_models[model_name]['preds'] = y_test_pred\n",
    "\n",
    "    # metrics\n",
    "    class_models[model_name]['Accuracy_train'] = accuracy_score(y_train_class, y_train_pred)\n",
    "    class_models[model_name]['Accuracy_test'] = accuracy_score(y_test_class, y_test_pred)\n",
    "    class_models[model_name]['Recall_train'] = recall_score(y_train_class, y_train_pred)\n",
    "    class_models[model_name]['Recall_test'] = recall_score(y_test_class, y_test_pred)\n",
    "    class_models[model_name]['precision_train'] = recall_score(y_train_class, y_train_pred)\n",
    "    class_models[model_name]['precision_test'] = recall_score(y_test_class, y_test_pred)\n",
    "    class_models[model_name]['F1_test'] = f1_score(y_test_class, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
