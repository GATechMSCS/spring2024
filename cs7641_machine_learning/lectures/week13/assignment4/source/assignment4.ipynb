{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_env_gs import (make_env, run_q_gridsearch, run_pi_gs, run_vi_gs)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Tuple, Discrete\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Frozen Lake}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = 'FrozenLake8x8-v1'\n",
    "is_slippery = True\n",
    "render_mode = 'ansi'\n",
    "prob_frozen = 0.8\n",
    "size = 16\n",
    "ep_steps = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFHFFFFFFFFF\n",
      "FFFFFHFFFFFFFFFF\n",
      "FFFFFHHFFFFFHFFH\n",
      "FFFHFFFFFFHFFFFF\n",
      "HFFFFFFHFFFFFFFF\n",
      "FFFFHHFFFFFHFFFF\n",
      "FFFFFFFFFFHFHFFF\n",
      "FHFFFFFHHFFFHHFF\n",
      "HFFFFHHFFFFFFFHF\n",
      "FFFFFHFFFFFFFHFF\n",
      "FFFFFHFHFFFFHHHF\n",
      "FFFFFFFHFHFFHFFF\n",
      "FFFFFFFFFFFHFFFF\n",
      "FFFFFFFFFFHFFHFH\n",
      "HFFFFFFFFHHFFHFF\n",
      "FFHFFFHFFFFFFFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frozenlake = make_env(mdp=mdp, size=size, slip=is_slippery,\n",
    "                      render=render_mode,\n",
    "                      prob_frozen=prob_frozen,\n",
    "                      seed=seed,\n",
    "                      ep_steps=ep_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_q = [0.90, 0.99, 0.999]\n",
    "epsilon_decay_q = [0.90, 0.99, 0.999]\n",
    "iters_q = [100000, 300000, 500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running q_learning with gamma: 0.9 epsilon decay: 0.9  iterations: 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leonardo_leads/miniconda3/envs/mlmdp/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/home/leonardo_leads/miniconda3/envs/mlmdp/lib/python3.12/site-packages/bettermdptools/algorithms/rl.py:178: UserWarning: Episode was truncated.  Bootstrapping 0 reward.\n",
      "  warnings.warn(\"Episode was truncated.  Bootstrapping 0 reward.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime = 80.32 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.9 epsilon decay: 0.9  iterations: 300000\n",
      "runtime = 246.20 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.9 epsilon decay: 0.9  iterations: 500000\n",
      "runtime = 411.91 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.9 epsilon decay: 0.99  iterations: 100000\n",
      "runtime = 77.94 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.9 epsilon decay: 0.99  iterations: 300000\n",
      "runtime = 238.98 seconds\n",
      "Avg. episode reward:  0.01\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.9 epsilon decay: 0.99  iterations: 500000\n",
      "runtime = 402.94 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.9 epsilon decay: 0.999  iterations: 100000\n",
      "runtime = 76.50 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.9 epsilon decay: 0.999  iterations: 300000\n",
      "runtime = 238.23 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.9 epsilon decay: 0.999  iterations: 500000\n",
      "runtime = 396.68 seconds\n",
      "Avg. episode reward:  0.07\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.9  iterations: 100000\n",
      "runtime = 103.48 seconds\n",
      "Avg. episode reward:  0.01\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.9  iterations: 300000\n",
      "runtime = 296.27 seconds\n",
      "Avg. episode reward:  0.02\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.9  iterations: 500000\n",
      "runtime = 489.03 seconds\n",
      "Avg. episode reward:  0.02\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.99  iterations: 100000\n",
      "runtime = 97.99 seconds\n",
      "Avg. episode reward:  0.07\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.99  iterations: 300000\n",
      "runtime = 284.12 seconds\n",
      "Avg. episode reward:  0.04\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.99  iterations: 500000\n",
      "runtime = 468.46 seconds\n",
      "Avg. episode reward:  0.09\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.999  iterations: 100000\n",
      "runtime = 99.74 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.999  iterations: 300000\n",
      "runtime = 285.66 seconds\n",
      "Avg. episode reward:  0.05\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.99 epsilon decay: 0.999  iterations: 500000\n",
      "runtime = 469.58 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.9  iterations: 100000\n",
      "runtime = 113.20 seconds\n",
      "Avg. episode reward:  0.01\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.9  iterations: 300000\n",
      "runtime = 331.10 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.9  iterations: 500000\n",
      "runtime = 541.56 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.99  iterations: 100000\n",
      "runtime = 107.58 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.99  iterations: 300000\n",
      "runtime = 317.36 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.99  iterations: 500000\n",
      "runtime = 523.87 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.999  iterations: 100000\n",
      "runtime = 108.85 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.999  iterations: 300000\n",
      "runtime = 316.47 seconds\n",
      "Avg. episode reward:  0.03\n",
      "###################\n",
      "\n",
      "running q_learning with gamma: 0.999 epsilon decay: 0.999  iterations: 500000\n",
      "runtime = 515.06 seconds\n",
      "Avg. episode reward:  0.02\n",
      "###################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_learning_results = run_q_gridsearch(process=frozenlake,\n",
    "                                      gamma=gamma_q,\n",
    "                                      epsilon_decay=epsilon_decay_q,\n",
    "                                      iters=iters_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learning_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_reward_one = {key: value for key, value in q_learning_results[(1.05, 1.0, 50000)].items() if value['average episode rewards'] == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, value in q_learning_results[(1.05, 1.0, 50000)].items():\n",
    "    # if key == 'average episode rewards':\n",
    "    #     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_learingFL = pd.DataFrame(data=q_learning_results[(1.05, 1.0, 50000)])\n",
    "# q_learning_results[(1.05, 1.0, 50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_p = [0.90, 0.95, 0.99, 0.999]\n",
    "theta_p = [0.01, 0.001, .0001, 0.00001]\n",
    "iters_p = [100000, 400000, 700000, 1_000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  0.28\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.29\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.26\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.28\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.29\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.26\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.28\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.29\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.26\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.28\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.29\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.9  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.26\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  0.18\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  0.25\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.33\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.18\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.25\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.12 seconds\n",
      "Avg. episode reward:  0.33\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.18\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.25\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.33\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.18\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.25\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.95  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.33\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.43\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.26 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.48 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.43\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.26 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.47 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.43\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.26 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.46 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.43\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.26 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.99  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.46 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.47\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.24 seconds\n",
      "Avg. episode reward:  0.57\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.67 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 100000  theta: 1e-05\n",
      "runtime = 1.27 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.47\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.24 seconds\n",
      "Avg. episode reward:  0.57\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.67 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 400000  theta: 1e-05\n",
      "runtime = 1.42 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.47\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.24 seconds\n",
      "Avg. episode reward:  0.57\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.69 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 700000  theta: 1e-05\n",
      "runtime = 1.30 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.47\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.24 seconds\n",
      "Avg. episode reward:  0.57\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.69 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running PI with gamma: 0.999  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 1.28 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pi_results = run_pi_gs(process=frozenlake,\n",
    "                       gamma=gamma_p,\n",
    "                       theta=theta_p,\n",
    "                       iters=iters_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0.9, 100000, 0.01), (0.9, 100000, 0.001), (0.9, 100000, 0.0001), (0.9, 100000, 1e-05), (0.9, 400000, 0.01), (0.9, 400000, 0.001), (0.9, 400000, 0.0001), (0.9, 400000, 1e-05), (0.9, 700000, 0.01), (0.9, 700000, 0.001), (0.9, 700000, 0.0001), (0.9, 700000, 1e-05), (0.9, 1000000, 0.01), (0.9, 1000000, 0.001), (0.9, 1000000, 0.0001), (0.9, 1000000, 1e-05), (0.95, 100000, 0.0001), (0.95, 100000, 1e-05), (0.95, 400000, 0.0001), (0.95, 400000, 1e-05), (0.95, 700000, 0.0001), (0.95, 700000, 1e-05), (0.95, 1000000, 0.0001), (0.95, 1000000, 1e-05), (0.99, 100000, 0.01), (0.99, 100000, 0.001), (0.99, 100000, 0.0001), (0.99, 100000, 1e-05), (0.99, 400000, 0.01), (0.99, 400000, 0.001), (0.99, 400000, 0.0001), (0.99, 400000, 1e-05), (0.99, 700000, 0.01), (0.99, 700000, 0.001), (0.99, 700000, 0.0001), (0.99, 700000, 1e-05), (0.99, 1000000, 0.01), (0.99, 1000000, 0.001), (0.99, 1000000, 0.0001), (0.99, 1000000, 1e-05), (0.999, 100000, 0.01), (0.999, 100000, 0.001), (0.999, 100000, 0.0001), (0.999, 100000, 1e-05), (0.999, 400000, 0.01), (0.999, 400000, 0.001), (0.999, 400000, 0.0001), (0.999, 400000, 1e-05), (0.999, 700000, 0.01), (0.999, 700000, 0.001), (0.999, 700000, 0.0001), (0.999, 700000, 1e-05), (0.999, 1000000, 0.01), (0.999, 1000000, 0.001), (0.999, 1000000, 0.0001), (0.999, 1000000, 1e-05)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_v = [0.90, 0.95, 0.99, 0.999]\n",
    "theta_v = [0.01, 0.001, .0001, 0.00001]\n",
    "iters_v = [100000, 400000, 700000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.28\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.11 seconds\n",
      "Avg. episode reward:  0.23\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.05 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.08 seconds\n",
      "Avg. episode reward:  0.28\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.23\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.28\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.11 seconds\n",
      "Avg. episode reward:  0.23\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.02 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.28\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.9  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.10 seconds\n",
      "Avg. episode reward:  0.23\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.20 seconds\n",
      "Avg. episode reward:  0.29\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.20 seconds\n",
      "Avg. episode reward:  0.29\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.19 seconds\n",
      "Avg. episode reward:  0.29\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.03 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.07 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.13 seconds\n",
      "Avg. episode reward:  0.27\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.95  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.20 seconds\n",
      "Avg. episode reward:  0.29\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.22 seconds\n",
      "Avg. episode reward:  0.48\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 0.0001\n",
      "runtime = 0.47 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 100000  theta: 1e-05\n",
      "runtime = 0.69 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.22 seconds\n",
      "Avg. episode reward:  0.48\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 0.0001\n",
      "runtime = 0.48 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 400000  theta: 1e-05\n",
      "runtime = 0.68 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.24 seconds\n",
      "Avg. episode reward:  0.48\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 0.0001\n",
      "runtime = 0.49 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 700000  theta: 1e-05\n",
      "runtime = 0.70 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.21 seconds\n",
      "Avg. episode reward:  0.48\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 0.48 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.99  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 0.68 seconds\n",
      "Avg. episode reward:  0.52\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.01\n",
      "runtime = 0.06 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.001\n",
      "runtime = 0.63 seconds\n",
      "Avg. episode reward:  0.57\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 0.0001\n",
      "runtime = 1.38 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 100000  theta: 1e-05\n",
      "runtime = 1.98 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.001\n",
      "runtime = 0.56 seconds\n",
      "Avg. episode reward:  0.57\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 0.0001\n",
      "runtime = 1.38 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 400000  theta: 1e-05\n",
      "runtime = 1.97 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.001\n",
      "runtime = 0.59 seconds\n",
      "Avg. episode reward:  0.57\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 0.0001\n",
      "runtime = 1.41 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 700000  theta: 1e-05\n",
      "runtime = 2.03 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.01\n",
      "runtime = 0.04 seconds\n",
      "Avg. episode reward:  0.0\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.001\n",
      "runtime = 0.57 seconds\n",
      "Avg. episode reward:  0.57\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 0.0001\n",
      "runtime = 1.38 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n",
      "running VI with gamma: 0.999  n_iters: 1000000  theta: 1e-05\n",
      "runtime = 2.00 seconds\n",
      "Avg. episode reward:  0.53\n",
      "###################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vi_results = run_vi_gs(process=frozenlake,\n",
    "                       gamma=gamma_v, \n",
    "                       theta=theta_v,\n",
    "                       iters=iters_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0.9, 100000, 0.0001), (0.9, 400000, 0.0001), (0.9, 700000, 0.0001), (0.9, 1000000, 0.0001), (0.95, 100000, 0.001), (0.95, 100000, 0.0001), (0.95, 100000, 1e-05), (0.95, 400000, 0.001), (0.95, 400000, 0.0001), (0.95, 400000, 1e-05), (0.95, 700000, 0.001), (0.95, 700000, 0.0001), (0.95, 700000, 1e-05), (0.95, 1000000, 0.001), (0.95, 1000000, 0.0001), (0.95, 1000000, 1e-05), (0.99, 100000, 0.001), (0.99, 100000, 0.0001), (0.99, 100000, 1e-05), (0.99, 400000, 0.001), (0.99, 400000, 0.0001), (0.99, 400000, 1e-05), (0.99, 700000, 0.001), (0.99, 700000, 0.0001), (0.99, 700000, 1e-05), (0.99, 1000000, 0.001), (0.99, 1000000, 0.0001), (0.99, 1000000, 1e-05), (0.999, 100000, 0.001), (0.999, 100000, 0.0001), (0.999, 100000, 1e-05), (0.999, 400000, 0.001), (0.999, 400000, 0.0001), (0.999, 400000, 1e-05), (0.999, 700000, 0.001), (0.999, 700000, 0.0001), (0.999, 700000, 1e-05), (0.999, 1000000, 0.001), (0.999, 1000000, 0.0001), (0.999, 1000000, 1e-05)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi_results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{Blackjack}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_bj = 'Blackjack-v1'\n",
    "render_mode_bj = 'ansi'\n",
    "size_bj = Tuple([Discrete(32), Discrete(11), Discrete(2)])\n",
    "ep_steps_bj = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackhjack = make_env_bj(mdp=mdp_bj,\n",
    "                            size=size_bj,\n",
    "                            render=render_mode_bj,\n",
    "                            seed=seed,\n",
    "                            ep_steps=ep_steps_bj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=[0.50, 0.75, .99]\n",
    "iters = [500, 750, 1000]\n",
    "theta=[.001, .00001]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlmdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
