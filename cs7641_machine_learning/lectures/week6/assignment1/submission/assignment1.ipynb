{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# statistical tests\n",
    "import scipy as sp\n",
    "\n",
    "# visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# model preparation\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
    "                                     GridSearchCV, RandomizedSearchCV,\n",
    "                                     StratifiedKFold, learning_curve)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# pytorch\n",
    "import torch as pt\n",
    "\n",
    "# machine learning models\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import (classification_report, ConfusionMatrixDisplay,\n",
    "                            RocCurveDisplay, accuracy_score, f1_score,\n",
    "                            precision_score, recall_score)\n",
    "\n",
    "np.random.seed(123)\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess for Cardiovascular Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"cardio_data_processed.csv\"\n",
    "cardio_vasc = pd.read_csv(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id: just an index\n",
    "# useing age_years over age\n",
    "# using weight and height instead of bmi\n",
    "# using ap_hi/lo instead of bp_category and bp_category_encoded\n",
    "cardio_cols_drop = ['id', 'age', 'bmi', 'bp_category', 'bp_category_encoded']\n",
    "cardio_vasc = cardio_vasc.drop(columns=cardio_cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_convert = ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio']\n",
    "cardio_vasc[cols_convert] = cardio_vasc[cols_convert].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 68205 entries, 0 to 68204\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   gender       68205 non-null  category\n",
      " 1   height       68205 non-null  int64   \n",
      " 2   weight       68205 non-null  float64 \n",
      " 3   ap_hi        68205 non-null  int64   \n",
      " 4   ap_lo        68205 non-null  int64   \n",
      " 5   cholesterol  68205 non-null  category\n",
      " 6   gluc         68205 non-null  category\n",
      " 7   smoke        68205 non-null  category\n",
      " 8   alco         68205 non-null  category\n",
      " 9   active       68205 non-null  category\n",
      " 10  cardio       68205 non-null  category\n",
      " 11  age_years    68205 non-null  int64   \n",
      "dtypes: category(7), float64(1), int64(4)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "cardio_vasc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>age_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  height  weight  ap_hi  ap_lo cholesterol gluc smoke alco active  \\\n",
       "0      2     168    62.0    110     80           1    1     0    0      1   \n",
       "1      1     156    85.0    140     90           3    1     0    0      1   \n",
       "2      1     165    64.0    130     70           3    1     0    0      0   \n",
       "\n",
       "  cardio  age_years  \n",
       "0      0         50  \n",
       "1      1         55  \n",
       "2      1         51  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardio_vasc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "cd_train, cd_test = train_test_split(cardio_vasc, test_size=.15, random_state=123)\n",
    "cd_train, cd_validate = train_test_split(cd_train, test_size=.2, random_state=123)\n",
    "\n",
    "# get X, y\n",
    "target_col=\"cardio\"\n",
    "cols_drop=[\"cardio\"]\n",
    "X_train_cd = cd_train.drop(cols_drop, axis=1)\n",
    "X_validate_cd = cd_validate.drop(cols_drop, axis=1)\n",
    "X_test_cd = cd_test.drop(cols_drop, axis=1)\n",
    "y_train_cd = cd_train[target_col]\n",
    "y_validate_cd = cd_validate[target_col]\n",
    "y_test_cd = cd_test[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Nutrition Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file2 = \"MyFoodData_Nutrition_Facts_SpreadSheet_Release_1.4.xlsx\"\n",
    "nutrition_facts = pd.read_excel(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts = nutrition_facts.dropna(axis=0, subset=[\"Food Group\"])\n",
    "\n",
    "# dropping columns that have every value missing\n",
    "cols_drop = [\"Added Sugar g\", \"Soluble Fiber g\", \"Insoluble Fiber g\",\n",
    "             \"Total sugar alcohols g\", \"Molybdenum mcg\", \"Chlorine mg\",\n",
    "             \"Biotin B7 mcg\", \"NetCarbs g\"]\n",
    "nutrition_facts = nutrition_facts.drop(columns=cols_drop, axis=1)\n",
    "\n",
    "# dropping cols that don't seem to mean much\n",
    "more_drop = [\"PRAL score\", \"ID\", \"Name\", '183 n3 ccc ALA mg',\n",
    "             '205 n3 EPA mg', '225 n3 DPA mg', '226 n3 DHA mg',\n",
    "             \"Serving Weight 1 g\", \"Serving Weight 2 g\", \"Serving Weight 3 g\",\n",
    "             \"Serving Weight 4 g\", \"Serving Weight 5 g\", \"Serving Weight 6 g\",\n",
    "             \"Serving Weight 7 g\", \"Serving Weight 8 g\", \"Serving Weight 9 g\",\n",
    "             \"200 Calorie Weight g\"]\n",
    "nutrition_facts = nutrition_facts.drop(columns=more_drop, axis=1)\n",
    "\n",
    "# drop column if 70% of its rows are empty\n",
    "threshold = int(.70*len(nutrition_facts))\n",
    "nutrition_facts.dropna(axis=1, thresh=threshold, inplace=True)\n",
    "\n",
    "# drop row if 70% of its columns are empty\n",
    "# threshold = int(.70*len(nutrition_facts.columns))\n",
    "# nutrition_facts.dropna(axis=0, thresh=threshold, inplace=True)\n",
    "\n",
    "nutrition_facts.fillna(0, inplace=True)\n",
    "\n",
    "nutrition_facts.columns = nutrition_facts.columns.str.lower()\n",
    "\n",
    "cols_rename = {\"food group\": \"food_group\", \"saturated fats g\": \"saturated_fats\",\n",
    "               \"fat g\": \"fat\", \"protein g\": \"protein\", \"carbohydrate g\": \"carbohydrate\",\n",
    "               \"sugars g\": \"sugars\", \"fiber g\": \"fiber\", \"cholesterol mg\": \"cholesterol\",\n",
    "               \"calcium mg\": \"calcium\", \"iron fe mg\": \"iron\", \"potassium k mg\": \"potassium\",\n",
    "               \"magnesium mg\": \"magnesium\", \"vitamin a rae mcg\": \"vitamin_a\", \"vitamin c mg\": \"vitamin_c\",\n",
    "               \"vitamin b12 mcg\": \"vitamin_b12\", \"vitamin d mcg\": \"vitamin_d\",\n",
    "               \"vitamin e alphatocopherol mg\": \"vitamin_e_alphatocopherol\", \"water g\": \"water\",\n",
    "               \"omega 3s mg\": \"omega_3s\", \"omega 6s mg\": \"omega_6s\", \"phosphorus p mg\": \"phosphorus\",\n",
    "               \"sodium mg\": \"sodium\", \"zinc zn mg\": \"zinc\", \"copper cu mg\": \"copper\",\n",
    "               \"selenium se mcg\": \"selenium\", \"thiamin b1 mg\": \"thiamin_b1\",\n",
    "               \"riboflavin b2 mg\": \"riboflavin_b2\", \"niacin b3 mg\": \"niacin_b3\",\n",
    "               \"vitamin b6 mg\": \"vitamin_b6\", \"folate b9 mcg\": \"folate_b9\",\n",
    "               \"folic acid mcg\": \"folic_acid\", \"food folate mcg\": \"food_folate\",\n",
    "               \"folate dfe mcg\": \"folate_dfe\", \"choline mg\": \"choline\", \"retinol mcg\": \"retinol\",\n",
    "               \"carotene beta mcg\": \"carotene_beta\", \"carotene alpha mcg\": \"carotene_alpha\",\n",
    "               \"lycopene mcg\": \"lycopene\", \"lutein + zeaxanthin mcg\": \"lutein_plus_zeaxanthin\",\n",
    "               \"vitamin k mcg\": \"vitamin_k\",\n",
    "               \"fatty acids total monounsaturated mg\": \"fatty_acids_total_monounsaturated\",\n",
    "               \"fatty acids total polyunsaturated mg\": \"fatty_acids_total_polyunsaturated\",\n",
    "               \"alcohol g\": \"alcohol\", \"caffeine mg\": \"caffeine\", \"theobromine mg\": \"theobromine\"}\n",
    "\n",
    "nutrition_facts = nutrition_facts.rename(mapper=cols_rename, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "nf_train, nf_test = train_test_split(nutrition_facts, test_size=.15, random_state=123)\n",
    "nf_train, nf_validate = train_test_split(nf_train, test_size=.2, random_state=123)\n",
    "\n",
    "# get X, y\n",
    "target_col=\"food_group\"\n",
    "cols_drop=[\"food_group\"]\n",
    "X_train_nf = nf_train.drop(cols_drop, axis=1)\n",
    "X_validate_nf = nf_validate.drop(cols_drop, axis=1)\n",
    "X_test_nf = nf_test.drop(cols_drop, axis=1)\n",
    "y_train_nf = nf_train[target_col]\n",
    "y_validate_nf = nf_validate[target_col]\n",
    "y_test_nf = nf_test[target_col]\n",
    "\n",
    "# scale data\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train_scaled_nf = pd.DataFrame(data=scale.transform(X_train_nf), columns=X_train_nf.columns, index=X_train_nf.index)\n",
    "X_validate_scaled_nf = pd.DataFrame(data=scale.transform(X_validate_nf), columns=X_validate_nf.columns, index=X_validate_nf.index)\n",
    "X_test_scaled_nf = pd.DataFrame(data=scale.transform(X_test_nf), columns=X_test_nf.columns, index=X_test_nf.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for Cardiovascular Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy Score: 0.51%\n"
     ]
    }
   ],
   "source": [
    "# calculat a baseline\n",
    "act_pred_error_cd = pd.DataFrame({\"actual\": y_train_cd})\n",
    "act_pred_error_cd[\"baseline_prediction\"] = y_train_cd.value_counts().index[0]\n",
    "\n",
    "baseline_acc_cd = accuracy_score(act_pred_error_cd[\"actual\"], act_pred_error_cd[\"baseline_prediction\"])\n",
    "\n",
    "# print baseline accuracy\n",
    "print(f\"Baseline Accuracy Score: {round(baseline_acc_cd, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline for Nutrition Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy Score: 0.23%\n"
     ]
    }
   ],
   "source": [
    "# calculat a baseline\n",
    "act_pred_error_nf = pd.DataFrame({\"actual\": y_train_nf})\n",
    "act_pred_error_nf[\"baseline_prediction\"] = y_train_nf.value_counts().index[0]\n",
    "\n",
    "baseline_acc_nf = accuracy_score(act_pred_error_nf[\"actual\"], act_pred_error_nf[\"baseline_prediction\"])\n",
    "\n",
    "# print baseline accuracy\n",
    "print(f\"Baseline Accuracy Score: {round(baseline_acc_nf, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Models for Both Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_models = {'decision tree':{'model': DecisionTreeClassifier(max_depth=7,\n",
    "                                                                 random_state=rand)},\n",
    "                'knn':{'model': KNeighborsClassifier(n_neighbors=7)},\n",
    "                'gradient boosting':{'model': GradientBoostingClassifier(n_estimators=210)},\n",
    "                'support vector machine': {'model': SVC()},\n",
    "                'mlp':{'model':make_pipeline(StandardScaler(),\n",
    "                                             MLPClassifier(hidden_layer_sizes=(7,),\n",
    "                                                           max_iter=500,\n",
    "                                                           early_stopping=True,\n",
    "                                                           random_state=rand))}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in class_models.keys():\n",
    "    fitted_model = class_models[model_name]['model'].fit(X_train, y_train_class)\n",
    "    y_train_pred = fitted_model.predict(X_train.values)\n",
    "    y_test_prob = fitted_model.predict_proba(X_test.values)[:,1]\n",
    "    y_test_pred = np.where(y_test_prob > 0.5, 1, 0)\n",
    "    class_models[model_name]['fitted'] = fitted_model\n",
    "    class_models[model_name]['probs'] = y_test_prob\n",
    "    class_models[model_name]['preds'] = y_test_pred\n",
    "\n",
    "    # metrics\n",
    "    class_models[model_name]['Accuracy_train'] = accuracy_score(y_train_class, y_train_pred)\n",
    "    class_models[model_name]['Accuracy_test'] = accuracy_score(y_test_class, y_test_pred)\n",
    "    class_models[model_name]['Recall_train'] = recall_score(y_train_class, y_train_pred)\n",
    "    class_models[model_name]['Recall_test'] = recall_score(y_test_class, y_test_pred)\n",
    "    class_models[model_name]['precision_train'] = recall_score(y_train_class, y_train_pred)\n",
    "    class_models[model_name]['precision_test'] = recall_score(y_test_class, y_test_pred)\n",
    "    class_models[model_name]['F1_test'] = f1_score(y_test_class, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import tqdm\n",
    "# from sklearn.metrics import roc_curve\n",
    "# from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Read data\n",
    "# data = pd.read_csv(\"sonar.csv\", header=None)\n",
    "# X = data.iloc[:, 0:60]\n",
    "# y = data.iloc[:, 60]\n",
    "\n",
    "# # Binary encoding of labels\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(y)\n",
    "# y = encoder.transform(y)\n",
    "\n",
    "# # Convert to 2D PyTorch tensors\n",
    "# X = torch.tensor(X.values, dtype=torch.float32)\n",
    "# y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# # Define two models\n",
    "# class Wide(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.hidden = nn.Linear(60, 180)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.output = nn.Linear(180, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.hidden(x))\n",
    "#         x = self.sigmoid(self.output(x))\n",
    "#         return x\n",
    "\n",
    "# class Deep(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layer1 = nn.Linear(60, 60)\n",
    "#         self.act1 = nn.ReLU()\n",
    "#         self.layer2 = nn.Linear(60, 60)\n",
    "#         self.act2 = nn.ReLU()\n",
    "#         self.layer3 = nn.Linear(60, 60)\n",
    "#         self.act3 = nn.ReLU()\n",
    "#         self.output = nn.Linear(60, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.act1(self.layer1(x))\n",
    "#         x = self.act2(self.layer2(x))\n",
    "#         x = self.act3(self.layer3(x))\n",
    "#         x = self.sigmoid(self.output(x))\n",
    "#         return x\n",
    "\n",
    "# # Compare model sizes\n",
    "# model1 = Wide()\n",
    "# model2 = Deep()\n",
    "# print(sum([x.reshape(-1).shape[0] for x in model1.parameters()]))  # 11161\n",
    "# print(sum([x.reshape(-1).shape[0] for x in model2.parameters()]))  # 11041\n",
    "\n",
    "# # Helper function to train one model\n",
    "# def model_train(model, X_train, y_train, X_val, y_val):\n",
    "#     # loss function and optimizer\n",
    "#     loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "#     n_epochs = 300   # number of epochs to run\n",
    "#     batch_size = 10  # size of each batch\n",
    "#     batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "#     # Hold the best model\n",
    "#     best_acc = - np.inf   # init to negative infinity\n",
    "#     best_weights = None\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         model.train()\n",
    "#         with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "#             bar.set_description(f\"Epoch {epoch}\")\n",
    "#             for start in bar:\n",
    "#                 # take a batch\n",
    "#                 X_batch = X_train[start:start+batch_size]\n",
    "#                 y_batch = y_train[start:start+batch_size]\n",
    "#                 # forward pass\n",
    "#                 y_pred = model(X_batch)\n",
    "#                 loss = loss_fn(y_pred, y_batch)\n",
    "#                 # backward pass\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 # update weights\n",
    "#                 optimizer.step()\n",
    "#                 # print progress\n",
    "#                 acc = (y_pred.round() == y_batch).float().mean()\n",
    "#                 bar.set_postfix(\n",
    "#                     loss=float(loss),\n",
    "#                     acc=float(acc)\n",
    "#                 )\n",
    "#         # evaluate accuracy at end of each epoch\n",
    "#         model.eval()\n",
    "#         y_pred = model(X_val)\n",
    "#         acc = (y_pred.round() == y_val).float().mean()\n",
    "#         acc = float(acc)\n",
    "#         if acc > best_acc:\n",
    "#             best_acc = acc\n",
    "#             best_weights = copy.deepcopy(model.state_dict())\n",
    "#     # restore model and return best accuracy\n",
    "#     model.load_state_dict(best_weights)\n",
    "#     return best_acc\n",
    "\n",
    "# # train-test split: Hold out the test set for final model evaluation\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "# # define 5-fold cross validation test harness\n",
    "# kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# cv_scores_wide = []\n",
    "# for train, test in kfold.split(X_train, y_train):\n",
    "#     # create model, train, and get accuracy\n",
    "#     model = Wide()\n",
    "#     acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "#     print(\"Accuracy (wide): %.2f\" % acc)\n",
    "#     cv_scores_wide.append(acc)\n",
    "# cv_scores_deep = []\n",
    "# for train, test in kfold.split(X_train, y_train):\n",
    "#     # create model, train, and get accuracy\n",
    "#     model = Deep()\n",
    "#     acc = model_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "#     print(\"Accuracy (deep): %.2f\" % acc)\n",
    "#     cv_scores_deep.append(acc)\n",
    "\n",
    "# # evaluate the model\n",
    "# wide_acc = np.mean(cv_scores_wide)\n",
    "# wide_std = np.std(cv_scores_wide)\n",
    "# deep_acc = np.mean(cv_scores_deep)\n",
    "# deep_std = np.std(cv_scores_deep)\n",
    "# print(\"Wide: %.2f%% (+/- %.2f%%)\" % (wide_acc*100, wide_std*100))\n",
    "# print(\"Deep: %.2f%% (+/- %.2f%%)\" % (deep_acc*100, deep_std*100))\n",
    "\n",
    "# # rebuild model with full set of training data\n",
    "# if wide_acc > deep_acc:\n",
    "#     print(\"Retrain a wide model\")\n",
    "#     model = Wide()\n",
    "# else:\n",
    "#     print(\"Retrain a deep model\")\n",
    "#     model = Deep()\n",
    "# acc = model_train(model, X_train, y_train, X_test, y_test)\n",
    "# print(f\"Final model accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # Test out inference with 5 samples\n",
    "#     for i in range(5):\n",
    "#         y_pred = model(X_test[i:i+1])\n",
    "#         print(f\"{X_test[i].numpy()} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n",
    "\n",
    "#     # Plot the ROC curve\n",
    "#     y_pred = model(X_test)\n",
    "#     fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "#     plt.plot(fpr, tpr) # ROC curve = TPR vs FPR\n",
    "#     plt.title(\"Receiver Operating Characteristics\")\n",
    "#     plt.xlabel(\"False Positive Rate\")\n",
    "#     plt.ylabel(\"True Positive Rate\")\n",
    "#     plt.show()\n",
    "\n",
    "### LOGISTIC REGRESSION\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# from sklearn import datasets\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# nn = torch.nn\n",
    "\n",
    "# # 0) prepare data\n",
    "# bc = datasets.load_breast_cancer()\n",
    "# x, y = bc.data, bc.target\n",
    "\n",
    "# n_samples, n_features = x.shape\n",
    "# print(n_samples, n_features)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=1234)\n",
    "\n",
    "# # scale\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "\n",
    "# X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "# X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "# y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "# y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "# y_train = y_train.view(y_train.shape[0], 1)\n",
    "# y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "# # 1) model\n",
    "# # f = wx + b, sigmoid at the end\n",
    "# class LogisticRegression(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_input_features):\n",
    "#         super(LogisticRegression, self).__init__()\n",
    "#         self.linear = nn.Linear(n_input_features, out_features=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y_pred = torch.sigmoid(self.linear(x))\n",
    "#         return y_pred\n",
    "\n",
    "# model = LogisticRegression(n_features)\n",
    "\n",
    "# # # 2) loss and optimizer\n",
    "# lr = .01\n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# # # 3) training loop\n",
    "# n_epochs = 100\n",
    "# for epoch in range(n_epochs):\n",
    "#     # forward pass and loss\n",
    "#     y_pred = model(X_train)\n",
    "#     loss = criterion(y_pred, y_train)\n",
    "\n",
    "#     # backward pass\n",
    "#     loss.backward()\n",
    "\n",
    "#     # update\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # zero gradients\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f\"epoch: {epoch+1}, loss = {loss.item()}\")\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     y_predicted = model(X_test)\n",
    "#     y_predicted_cls = y_predicted.round()\n",
    "#     acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "#     print(f\"accuracy = {acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
